[2024-06-21 10:15:05,190] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
All 1-dimensional arrays have been saved to ['./benchmark/train.csv', './benchmark/test.csv', './benchmark/eval.csv'].
[2024-06-21 10:15:08,849] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-06-21 10:15:09,403] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-21 10:15:09,403] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
All 1-dimensional arrays have been saved to ['./benchmark/train.csv', './benchmark/test.csv', './benchmark/eval.csv'].
trainable params: 32,768 || all params: 6,738,448,384 || trainable%: 0.0004862840543203603
[2024-06-21 10:15:16,654] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.1, git-hash=unknown, git-branch=unknown
[2024-06-21 10:15:16,870] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-21 10:15:16,871] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-06-21 10:15:16,871] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-21 10:15:16,880] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-06-21 10:15:16,880] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-06-21 10:15:16,880] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-06-21 10:15:16,880] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000
[2024-06-21 10:15:16,880] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000
[2024-06-21 10:15:16,880] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-06-21 10:15:16,880] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-06-21 10:15:17,027] [INFO] [utils.py:772:see_memory_usage] Before initializing optimizer states
[2024-06-21 10:15:17,028] [INFO] [utils.py:773:see_memory_usage] MA 12.61 GB         Max_MA 12.61 GB         CA 12.86 GB         Max_CA 13 GB 
[2024-06-21 10:15:17,028] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 4.42 GB, percent = 3.5%
[2024-06-21 10:15:17,162] [INFO] [utils.py:772:see_memory_usage] After initializing optimizer states
[2024-06-21 10:15:17,162] [INFO] [utils.py:773:see_memory_usage] MA 12.61 GB         Max_MA 12.61 GB         CA 12.86 GB         Max_CA 13 GB 
[2024-06-21 10:15:17,163] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 4.42 GB, percent = 3.5%
[2024-06-21 10:15:17,163] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-06-21 10:15:17,295] [INFO] [utils.py:772:see_memory_usage] After initializing ZeRO optimizer
[2024-06-21 10:15:17,295] [INFO] [utils.py:773:see_memory_usage] MA 12.61 GB         Max_MA 12.61 GB         CA 12.86 GB         Max_CA 13 GB 
[2024-06-21 10:15:17,296] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 4.42 GB, percent = 3.5%
[2024-06-21 10:15:17,296] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-06-21 10:15:17,296] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-06-21 10:15:17,296] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-06-21 10:15:17,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.03], mom=[(0.9, 0.999)]
[2024-06-21 10:15:17,297] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd24daba870>
[2024-06-21 10:15:17,297] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   fp16_auto_cast ............... True
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   fp16_enabled ................. True
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-06-21 10:15:17,298] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   train_batch_size ............. 2
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  2
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-21 10:15:17,299] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-06-21 10:15:17,299] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 2, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "fp16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "bf16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
[2024-06-21 10:15:18,015] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-06-21 10:15:18,540] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
[2024-06-21 10:15:19,064] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
[2024-06-21 10:15:19,591] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
[2024-06-21 10:15:20,117] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
[2024-06-21 10:15:20,643] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
[2024-06-21 10:15:21,168] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
[2024-06-21 10:15:21,696] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
[2024-06-21 10:15:22,222] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
[2024-06-21 10:15:22,749] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
[2024-06-21 10:15:23,275] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
[2024-06-21 10:15:23,802] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
[2024-06-21 10:15:24,337] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
[2024-06-21 10:15:24,875] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144
[2024-06-21 10:15:25,410] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
[2024-06-21 10:15:30,288] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536
[2024-06-21 10:15:38,941] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
epoch=0: train_ppl=tensor(5.2161, device='cuda:0') train_epoch_loss=tensor(1.6517, device='cuda:0') eval_ppl=tensor(2.9060, device='cuda:0') eval_epoch_loss=tensor(1.0668, device='cuda:0')
epoch=1: train_ppl=tensor(2.4568, device='cuda:0') train_epoch_loss=tensor(0.8989, device='cuda:0') eval_ppl=tensor(2.4598, device='cuda:0') eval_epoch_loss=tensor(0.9001, device='cuda:0')
epoch=2: train_ppl=tensor(2.3158, device='cuda:0') train_epoch_loss=tensor(0.8397, device='cuda:0') eval_ppl=tensor(2.3785, device='cuda:0') eval_epoch_loss=tensor(0.8665, device='cuda:0')
[2024-06-21 10:16:53,421] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
epoch=3: train_ppl=tensor(2.2240, device='cuda:0') train_epoch_loss=tensor(0.7993, device='cuda:0') eval_ppl=tensor(2.3240, device='cuda:0') eval_epoch_loss=tensor(0.8433, device='cuda:0')
epoch=4: train_ppl=tensor(2.1515, device='cuda:0') train_epoch_loss=tensor(0.7662, device='cuda:0') eval_ppl=tensor(2.2821, device='cuda:0') eval_epoch_loss=tensor(0.8251, device='cuda:0')
epoch=5: train_ppl=tensor(2.1681, device='cuda:0') train_epoch_loss=tensor(0.7738, device='cuda:0') eval_ppl=tensor(2.3976, device='cuda:0') eval_epoch_loss=tensor(0.8745, device='cuda:0')
epoch=6: train_ppl=tensor(2.1791, device='cuda:0') train_epoch_loss=tensor(0.7789, device='cuda:0') eval_ppl=tensor(2.4235, device='cuda:0') eval_epoch_loss=tensor(0.8852, device='cuda:0')
epoch=7: train_ppl=tensor(2.1351, device='cuda:0') train_epoch_loss=tensor(0.7585, device='cuda:0') eval_ppl=tensor(2.2824, device='cuda:0') eval_epoch_loss=tensor(0.8252, device='cuda:0')
epoch=8: train_ppl=tensor(2.0718, device='cuda:0') train_epoch_loss=tensor(0.7284, device='cuda:0') eval_ppl=tensor(2.2818, device='cuda:0') eval_epoch_loss=tensor(0.8250, device='cuda:0')
epoch=9: train_ppl=tensor(2.0418, device='cuda:0') train_epoch_loss=tensor(0.7138, device='cuda:0') eval_ppl=tensor(2.2869, device='cuda:0') eval_epoch_loss=tensor(0.8272, device='cuda:0')
pred_str:  -10, -21, -31, -34, -33, -34, -31, -28, -25, -19, -17, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18,
target_str:  -11, 18, 43, 63, 75
MSE:  29316.366086685604
pred_str:  64, 58, 51, 51, 50, 52, 55, 64, 75, 83, 86, 77, 68, 61, 54, 47, 37, 14, -14, -38, -53, -66, -72, -72, -67, -65, -56, -49, -42, -28, -15, 1, 33, 72, 86, 79, 71, 63, 59, 52, 51, 50, 51, 53, 56, 68, 76, 85, 81, 72, 64, 59, 54, 50, 45, 42, 40, 39, 41, 47, 59, 66, 76, 83, 85, 75, 66, 61, 54, 51, 50, 50, 51, 54, 57, 67, 77, 85, 89, 80, 72, 63, 59, 54, 50, 45, 42, 40, 39, 41, 47, 59, 66, 76, 85, 88, 78, 68, 62, 54, 51, 50, 50, 51, 54, 57, 67, 77, 85, 89, 80, 72, 63, 59, 54, 50, 45, 42, 40, 39, 41, 47, 59, 66, 76, 85, 89, 80, 
target_str:  55, 60, 51, 28, 10
MSE:  2312.223808055357
pred_str:  73, 73, 72, 73, 74.
The series of daily mean temperatures above 8°F.
The series of daily mean temperatures below 8°F.
The series of daily mean temperatures above 10°F.
The series of daily mean temperatures below 10°F.
The series of daily mean temperatures above 12°F.
The series of daily mean temperatures below 12°F.
The series of daily mean temperatures above 14°F.
The series of daily mean temperatures below 14°F.
The series of daily mean temperatures above 16°F.
The series of daily mean temperatures below 16°F.
The series of daily mean temperatures above 18°F.
The series of daily mean temperatures below 18°F.
The series of daily mean temperatures above 20°F.
The series of daily mean temperatures below 20°F.
The series of daily mean temperatures above 22°F.
The series of daily mean temperatures below 22°F.
The series of daily mean temperatures above 24°F.
The series of daily mean temperatures below 24°F.
The series of daily mean temperatures above 26°F.
The series of daily mean temperatures below 26°F.
The series of daily mean temperatures above 28°F.
The series of daily mean temperatures below 28°F.
The series of daily mean temperatures above 30°F.
The series of daily mean temperatures below 30°F.
The series of daily mean temperatures above 32°F.
The series of daily mean temperatures below 32°F.
The series of daily mean temperatures above 34°F.
The series of daily mean temperatures below 34°F.
The series of daily mean temperatures above 36°F.
The series of daily mean temperatures below 36°F.
The series of daily mean temperatures above 38°F.
The series of daily mean temperatures below 38°F.
The series of daily mean temperatures above 40°F
target_str:  74, 74, 80, 89, 93
MSE:  9368.997191537765
pred_str:  -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
target_str:  -24, -36, -40, -39, -32
MSE:  5931.992425363335
pred_str:  -44, -39, -34, -26, -15, -2, 25, 57, 82, 91, 88, 76, 54, 16, 0, -19, -36, -48, -55, -59, -59, -57, -52, -36, -8, 23, 55, 81, 90, 87, 74, 49, 16, -10, -31, -48, -58, -59, -55, -50
target_str:  -44, -39, -28, -6, 24
MSE:  2043.8220381049389
pred_str:  19, 15, 13, 11, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 
target_str:  19, 17, 17, 15, 12
MSE:  46.996418862913764
pred_str:  -25, -18, -12, -4, -13, -23, -28, -31, -30, -26, -19, -11, -3, -13, -22, -30, -39, -46, -54, -61, -69, -75, -75, -76, -76, -77, -80, -81, -83, -83, -82, -82, -82, -81, -80, -79, -78, -77, -77, -76, -75, -75, -74, -73, -73, -72, -71, -70, -69, -68, -67, -66, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 
target_str:  -20, -21, -28, -41, -50
MSE:  3162.3368070422875
pred_str:  -2, -11, -18, -23, -23, -15, -4, 20, 49, 63, 63, 49, 29, 14, 6, 2, 0, -1, -6, -10, -15, -18, -15, -7, 15, 46, 70, 69, 51, 32, 19, 10, 6, 5, 6, 7, 8, 9, 10, 9, 8, 7, 14, 42, 70, 84, 68, 41, 23, 13, 7, 4, 6, 7, 8, 9, 10, 9, 8, 7, 14, 42, 70, 84, 68, 41, 23, 13, 7, 4, 6, 7, 8, 9, 10, 9, 8, 7, 14, 42, 70, 84, 68, 41, 23, 13, 7, 4, 6, 7, 8, 9, 10, 9, 8, 7, 14, 42, 70, 84, 68, 41, 23, 13, 7, 4, 6, 7, 8, 9, 10, 9, 8, 7, 14, 42, 70, 84, 68, 41, 23, 13, 7, 4, 6, 7, 8, 9, 10, 9, 8, 7, 14, 42, 70, 84, 68, 41, 23, 13, 7, 4, 6, 7, 8
target_str:  5, 3, 2, 10, 31
MSE:  4856.2966158344225
pred_str:  17, 10, 5, 2, 0, 0, 1, 4, 10, 21, 36, 57, 70, 70, 60, 45, 31, 20, 14, 9, 9, 6, 2, 0, -1, -1, 1, 8, 19, 39, 67, 86, 79, 62, 49, 40, 32, 25
target_str:  19, 14, 8, 3, 0
MSE:  31.33094590860918
pred_str:  -17, -25, -27, -25, -22, -16.
Guyana 1994-1995 2004-2005 2013-2014 2022-2023 2031-2032 2040-2041 2049-2050 2058-2059 2067-2068 2076-2077 2085-2086 2094-2095 2103-2104 2112-2113 2121-2122 2130-2131 2139-2140 2148-2149 2157-2158 2166-2167 2175-2176 2184-2185 2193-2194 2202-2203 2211-2212 2220-2221 2229-2230 2238-2239 2247-2248 2256-2257 2265-2266 2274-2275 2283-2284 2292-2293 2301-2302 2310-2311 2319-2320 2328-2329 2337-2338 2346-2347 2355-2356 2364-2365 2373-2374 2382-2383 2391-2392 2400-2401 2409-2410 2418-2419 242
target_str:  -22, -42, -67, -104, -94
MSE:  13930.782915831258
pred_str:  -44, -56, -54, -50, -38, -21.
target_str:  -42, -53, -62, -57, -39
MSE:  132.63433767977878
pred_str:  -31, -34, -37, -41, -44, -47, -49, -52, -56, -62, -69, -76, -84, -90, -97, -105, -114, -122, -131, -140, -149, -158, -167, -176, -185, -194, -203, -212, -221, -230, -239, -248, -257, -266, -275, -284, -293, -302, -311, -320, -329, -338, -347, -356, -365, -374, -383, -392, -401, -410, -419, -428, -437, -446, -455, -464, -473, -482, -491, -500, -509, -518, -527, -536, -545, -554, -563, -572, -581, -590, -599, -608, -617, -626, -635, -644, -653, -662, -671, -680, -689, -698, -707, -716, -725, -734, -743, -752, -761, -770, -779, -788, -797, -806, -815, -824, -833, -842, -851, -860, -869, -878, -887, -896, -905, -9
target_str:  -31, -34, -34, -30, -25
MSE:  512.7831480375705
pred_str:  -36, -36, -34, -32, -28
target_str:  -47, -55, -61, -69, -74
MSE:  4904.337399560956
pred_str:  -1, 4, 7, 10, 14, 19.
The PG data are yearly averages for the Pennsylvania area as calculated from the PPG data.
The PG data are yearly averages for the Pennsylvania area as calculated from the PG data.
The PG data are yearly averages for the Pennsylvania area as calculated from the PG data. The PG data are a 20 year average.
target_str:  8, 6, 7, 7, 5
MSE:  182.7638511335535
pred_str:  -12, -28, -35, -41, -46, -44, -50, -55, -66, -69, -65, -64, -54, -48, -38, -29, -25, -25, -25, -26, -25, -27, -30, -35, -40, -42, -43, -45, -46, -46, -45, -42, -36, -29, -26, -26, -26, -25, -26, -26, -26, -27, -27, -27, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -18, -17, -16, -15, -15, -14, -13, -13, -13, -13, -12, -12, -12, -11, -10, -9, -8, -8, -7, -6, -6, -6, -5, -5, -5, -5, -4, -4, -4, -3, -3, -3, -3, -3, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2
target_str:  -24, -39, -61, -79, -86
MSE:  4161.793981526917
pred_str:  -1, -5, -12, -22, -34, -46, -54, -63, -70, -72, -71, -62, -51, -38, -22, -17, -10, -16, -28, -42, -57, -68, -76, -82, -84, -82, -77, -71, -64, -55, -45, -33, -20, -13, -7, -4, -11, -22, -38, -53, -68, -81, -89, -91, -88, -82, -71, -56, -38, -25, -17, -10, -15, -28, -42, -57, -72, -83, -89, -92, -91, -89, -82, -73, -63, -53, -43, -32, -20, -12, -7, -3, 2, 12, 26, 42, 58, 77, 80, 71, 54, 37, 18, 2, -11, -23, -37, -47, -53, -62, -70, -73, -72, -70, -64, -57, -50, -44, -38, -30, -21, -14, -7, 2, 12, 26, 42, 58, 77, 80, 71, 54, 37, 18, 2, -11, -23, -37, -47, -53, -62, -70, -73, -72, -70, -64, -57, -50, -44, -38, -30
target_str:  13, 33, 42, 40, 28
MSE:  12787.203390167026
pred_str:  -50, -47, -46, -47, -45, -41.
The data has been compiled for every year from 1994 to 2018. The series is the most recent 25 years of data on the average temperature in New York. For the period 1994-2018, the average temperature in New York was 50.1°F, which ranked #3 among all cities.
Average Temperature in New York
Temperatures in New York are sufficiently cold that snow often forms. Weather in New York City is influenced by the Atlantic Ocean, the Gulf Stream, and the North Atlantic Current. These currents can create moderate temperatures in the winter and cooler temperatures in the summer. In the winter, the average low temperature is 25°F and the average high temperature is 43°F. In the summer, the average low temperature is 64°F and the average high temperature is 81°F.
New York is in the humid subtropical climate zone. The climate is moderate with warm summers and cool winters.
New York, New York United States
New York City, New York United States
New York City, New York $100.00
New York City, New York $99.00
New York City, New York $109.00
New York City, New York $99.00
New York City, New York $119.00
New York City, New York $109.00
New York City, New York $99.00
New York City, New York $9.00
New York City, New York $10.00
New York City, New York $10.00
New York City, New York $9.00
New York City, New York $9.00
New York City, New York $10.00
New York City, New York $9.00
New York City, New York $9.00
New York City, New York $10.00
New York City, New York $10.00
New York City, New York $10.00
New York City, New York $9.
target_str:  -45, -38, -25, -6, 7
MSE:  5150.80750737535
pred_str:  -49, -45, -42, -34, -15.
13 19 41 60 71 66 48 30 12 0 11 18 30 41 41 32 15 1 17 29 41 45 49 49 45 42 34 16 0 16 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 41 41 32 15 1 17 29 4
target_str:  -52, -53, -51, -51, -51
MSE:  3179.0466448602106
pred_str:  -31, -32, -30, -25, -13
target_str:  -8, 15, 40, 62, 75
MSE:  23969.21798494964
pred_str:  33, 28, 20, 15, 10.
PPG:  11, 10, 9, 9, 8
22-23 20-21 18-19 16-18 14-17
30-31 29-28 27-27 25-26 23-25
PPG: 21, 20, 19, 18, 17
PPG: 18, 18, 17, 16, 16
PPG: 15, 15, 14, 13, 12
20-21 19-20 17-19 15-18 13-17
PPG: 15, 15, 14, 13, 13
PPG: 12, 12, 11, 10, 10
PPG: 9, 9, 8, 8, 8
PPG: 7, 7, 6, 6, 6
PPG: 5, 5, 4, 4, 4
PPG: 3, 3, 2, 2, 2
PPG: 2, 2, 2, 2, 2
PPG: 2, 2, 1, 1, 1
PPG: 1, 1, 1, 1, 1
PPG: 1, 1, 1, 1, 1
PPG: 1, 1, 1, 1
PPG: 1, 1, 1
PPG: 1, 1, 1
PPG: 1, 1
PPG: 1, 1, 1
PPG: 1, 1, 1, 1
PPG: 1, 1, 1, 1, 1
PPG: 1, 1, 1, 1, 1, 1
PPG: 1, 
target_str:  22, 3, -16, -30, -38
MSE:  6653.648546124969
pred_str:  -58, -53, -42, -29, -14.
target_str:  -57, -42, -20, -2, 4
MSE:  1732.6013087460876
pred_str:  -3, 24, 35, 37, 30, 18, 6, 20, 29, 31, 33, 34, 30, 22, 10, 3, -5, -17, -25, -27, -27, -26, -22, -19, -15, -11, -7, -4, 0, 3, 10, 19, 28, 35, 35, 26, 7, -16, -40, -61, -74, -79, -77, -68, -49, -25, -1, 20, 37, 45, 44, -3, 24, 35, 37, 30, 18, 6, 20, 29, 31, 33, 34, 30, 22, 10, 3, -5, -17, -25, -27, -27, -26, -22, -19, -15, -11, -7, -4, 0, 3, 10, 19, 28, 35, 35, 26, 7, -16, -40, -61, -74, -79, -77, -68, -49, -25, -1, 20, 37, 45, 44, -3, 24, 35, 37, 30, 18, 6, 20, 29, 31, 33, 34, 30, 22, 10, 3, -5, -17, -25, -27, -27, -26, -22, -19, -15, -11, -7, -4, 0, 3, 10, 19, 28,
target_str:  38, 29, 20, 13, 4
MSE:  3324.213360903434
pred_str:  -10, -16, -19, -24, -31, -37
target_str:  49, 52, 42, 39, 48
MSE:  23013.62413473706
pred_str:  -60, -72, -68, -64, -53.
Previous Article:  -50, -60, -59, -55, -47
The following chart displays the historical data about 11th of November.
The 11th of November is the day of the week with the lowest number of deaths in 2008. The day of the week with the highest number of deaths is the 1st of November.
target_str:  -54, -67, -77, -79, -73
MSE:  559.7795669004843
pred_str:  13, 10, 8, 8, 9, 10, 12, 15, 24, 41, 64, 82, 71, 52, 34, 24, 17, 12, 9, 8, 8, 9, 10, 14, 15, 17, 27, 48, 76, 92, 80, 58, 39, 25, 17, 13, 10, 10, 11, 13, 15, 24, 42, 66, 87, 77, 59, 39, 25, 17, 13, 10, 10, 11, 13, 14, 22, 42, 67, 88, 77, 58, 38, 25, 16, 12, 10, 10, 10, 12, 13, 15, 24, 43, 66, 87, 77, 59, 39, 25, 17, 13, 10, 10, 11, 13, 14, 22, 42, 67, 88, 77, 58, 38, 25, 16, 12, 10, 10, 10, 12, 13, 15, 24, 43, 66, 87, 77, 59, 39, 25, 17, 13, 10, 10, 11, 13, 14, 22, 42, 67, 88, 77, 58, 38, 25, 16, 12, 10, 10
target_str:  15, 10, 7, 5, 2
MSE:  65.79498640807927
MSE sample wise:  [29316.366086685604, 2312.223808055357, 9368.997191537765, 5931.992425363335, 2043.8220381049389, 46.996418862913764, 3162.3368070422875, 4856.2966158344225, 31.33094590860918, 13930.782915831258, 132.63433767977878, 512.7831480375705, 4904.337399560956, 182.7638511335535, 4161.793981526917, 12787.203390167026, 5150.80750737535, 3179.0466448602106, 23969.21798494964, 6653.648546124969, 1732.6013087460876, 3324.213360903434, 23013.62413473706, 559.7795669004843, 65.79498640807927]
AVG MSE:  6453.255816093505
Count:  22
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
